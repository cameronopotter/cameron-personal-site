name: 🚀 Performance Monitoring & Optimization

on:
  schedule:
    # Run performance monitoring every 4 hours
    - cron: '0 */4 * * *'
  push:
    branches: [ main ]
    paths:
      - 'frontend/**'
      - 'backend/**'
      - '.github/workflows/performance-monitoring.yml'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test'
        required: true
        default: 'lighthouse'
        type: choice
        options:
          - lighthouse
          - load-test
          - bundle-analysis
          - api-performance
          - full-suite
      environment:
        description: 'Environment to test'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production

env:
  STAGING_URL: 'https://staging.digital-greenhouse.dev'
  PRODUCTION_URL: 'https://digital-greenhouse.dev'
  STAGING_API_URL: 'https://staging-api.digital-greenhouse.dev'
  PRODUCTION_API_URL: 'https://api.digital-greenhouse.dev'

jobs:
  # ===================================
  # Lighthouse Performance Audit
  # ===================================
  
  lighthouse-audit:
    name: 🔍 Lighthouse Performance Audit
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'schedule' || 
      github.event.inputs.test_type == 'lighthouse' ||
      github.event.inputs.test_type == 'full-suite' ||
      contains(github.event.head_commit.message, '[perf-test]')
    
    strategy:
      matrix:
        device: [desktop, mobile]
        environment: 
          - ${{ github.event.inputs.environment || 'staging' }}
    
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4
      
      - name: 🟢 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
      
      - name: 📦 Install Lighthouse CI
        run: npm install -g @lhci/cli
      
      - name: 🔧 Set environment URLs
        id: set-urls
        run: |
          if [ "${{ matrix.environment }}" = "production" ]; then
            echo "BASE_URL=${{ env.PRODUCTION_URL }}" >> $GITHUB_OUTPUT
            echo "API_URL=${{ env.PRODUCTION_API_URL }}" >> $GITHUB_OUTPUT
          else
            echo "BASE_URL=${{ env.STAGING_URL }}" >> $GITHUB_OUTPUT
            echo "API_URL=${{ env.STAGING_API_URL }}" >> $GITHUB_OUTPUT
          fi
      
      - name: 🚀 Run Lighthouse audit
        run: |
          lhci autorun \
            --upload.target=temporary-public-storage \
            --collect.url="${{ steps.set-urls.outputs.BASE_URL }}" \
            --collect.url="${{ steps.set-urls.outputs.BASE_URL }}/projects" \
            --collect.url="${{ steps.set-urls.outputs.BASE_URL }}/skills" \
            --collect.settings.chromeFlags="--no-sandbox" \
            --collect.settings.preset="${{ matrix.device }}" \
            --collect.numberOfRuns=3 \
            --assert.assertions.performance=0.8 \
            --assert.assertions.accessibility=0.9 \
            --assert.assertions.best-practices=0.8 \
            --assert.assertions.seo=0.8 \
            --outputDir=./lighthouse-results \
            --collect.settings.outputPath=./lighthouse-results/report-${{ matrix.device }}-${{ matrix.environment }}.json
        continue-on-error: true
      
      - name: 📊 Parse Lighthouse results
        id: lighthouse-results
        run: |
          # Extract key metrics from Lighthouse results
          PERFORMANCE=$(jq -r '.audits.performance.score * 100' lighthouse-results/report-${{ matrix.device }}-${{ matrix.environment }}.json)
          ACCESSIBILITY=$(jq -r '.audits.accessibility.score * 100' lighthouse-results/report-${{ matrix.device }}-${{ matrix.environment }}.json)
          BEST_PRACTICES=$(jq -r '.audits["best-practices"].score * 100' lighthouse-results/report-${{ matrix.device }}-${{ matrix.environment }}.json)
          SEO=$(jq -r '.audits.seo.score * 100' lighthouse-results/report-${{ matrix.device }}-${{ matrix.environment }}.json)
          FCP=$(jq -r '.audits["first-contentful-paint"].numericValue' lighthouse-results/report-${{ matrix.device }}-${{ matrix.environment }}.json)
          LCP=$(jq -r '.audits["largest-contentful-paint"].numericValue' lighthouse-results/report-${{ matrix.device }}-${{ matrix.environment }}.json)
          
          echo "performance=${PERFORMANCE}" >> $GITHUB_OUTPUT
          echo "accessibility=${ACCESSIBILITY}" >> $GITHUB_OUTPUT
          echo "best_practices=${BEST_PRACTICES}" >> $GITHUB_OUTPUT
          echo "seo=${SEO}" >> $GITHUB_OUTPUT
          echo "fcp=${FCP}" >> $GITHUB_OUTPUT
          echo "lcp=${LCP}" >> $GITHUB_OUTPUT
      
      - name: 📈 Create performance report
        run: |
          cat > performance-report-${{ matrix.device }}-${{ matrix.environment }}.md << EOF
          # 🚀 Performance Report - ${{ matrix.device }} - ${{ matrix.environment }}
          
          **Date**: $(date)
          **URL**: ${{ steps.set-urls.outputs.BASE_URL }}
          **Device**: ${{ matrix.device }}
          
          ## Lighthouse Scores
          
          | Metric | Score |
          |--------|-------|
          | Performance | ${{ steps.lighthouse-results.outputs.performance }}% |
          | Accessibility | ${{ steps.lighthouse-results.outputs.accessibility }}% |
          | Best Practices | ${{ steps.lighthouse-results.outputs.best_practices }}% |
          | SEO | ${{ steps.lighthouse-results.outputs.seo }}% |
          
          ## Core Web Vitals
          
          | Metric | Value |
          |--------|-------|
          | First Contentful Paint | ${{ steps.lighthouse-results.outputs.fcp }}ms |
          | Largest Contentful Paint | ${{ steps.lighthouse-results.outputs.lcp }}ms |
          
          ## Recommendations
          
          EOF
          
          # Add performance recommendations based on scores
          if (( $(echo "${{ steps.lighthouse-results.outputs.performance }}" '<' 80 | bc -l) )); then
            echo "- ⚠️ Performance score is below 80%. Consider optimizing bundle size and render-blocking resources." >> performance-report-${{ matrix.device }}-${{ matrix.environment }}.md
          fi
          
          if (( $(echo "${{ steps.lighthouse-results.outputs.lcp }}" '>' 2500 | bc -l) )); then
            echo "- ⚠️ LCP is above 2.5s. Optimize image loading and critical rendering path." >> performance-report-${{ matrix.device }}-${{ matrix.environment }}.md
          fi
      
      - name: 📤 Upload Lighthouse results
        uses: actions/upload-artifact@v3
        with:
          name: lighthouse-results-${{ matrix.device }}-${{ matrix.environment }}
          path: |
            lighthouse-results/
            performance-report-${{ matrix.device }}-${{ matrix.environment }}.md
          retention-days: 30
      
      - name: 🚨 Performance alert
        if: |
          steps.lighthouse-results.outputs.performance < 70 ||
          steps.lighthouse-results.outputs.lcp > 3000
        uses: 8398a7/action-slack@v3
        with:
          status: custom
          custom_payload: |
            {
              "text": "⚠️ Performance Alert - Digital Greenhouse",
              "attachments": [{
                "color": "warning",
                "fields": [
                  {"title": "Environment", "value": "${{ matrix.environment }}", "short": true},
                  {"title": "Device", "value": "${{ matrix.device }}", "short": true},
                  {"title": "Performance Score", "value": "${{ steps.lighthouse-results.outputs.performance }}%", "short": true},
                  {"title": "LCP", "value": "${{ steps.lighthouse-results.outputs.lcp }}ms", "short": true}
                ]
              }]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}

  # ===================================
  # API Performance Testing
  # ===================================
  
  api-performance:
    name: ⚡ API Performance Testing
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'schedule' || 
      github.event.inputs.test_type == 'api-performance' ||
      github.event.inputs.test_type == 'full-suite'
    
    strategy:
      matrix:
        environment: 
          - ${{ github.event.inputs.environment || 'staging' }}
    
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4
      
      - name: 🔧 Set environment URLs
        id: set-urls
        run: |
          if [ "${{ matrix.environment }}" = "production" ]; then
            echo "API_URL=${{ env.PRODUCTION_API_URL }}" >> $GITHUB_OUTPUT
          else
            echo "API_URL=${{ env.STAGING_API_URL }}" >> $GITHUB_OUTPUT
          fi
      
      - name: 📦 Install k6
        run: |
          curl -s https://github.com/grafana/k6/releases/download/v0.46.0/k6-v0.46.0-linux-amd64.tar.gz | tar xvz --strip-components 1
          sudo mv k6 /usr/local/bin/
      
      - name: ⚡ Run API performance tests
        run: |
          cat > api-perf-test.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Rate, Trend } from 'k6/metrics';
          
          const errorRate = new Rate('errors');
          const responseTime = new Trend('response_time');
          
          export const options = {
            stages: [
              { duration: '2m', target: 20 },
              { duration: '5m', target: 50 },
              { duration: '2m', target: 0 },
            ],
            thresholds: {
              http_req_duration: ['p(95)<500', 'p(99)<1000'],
              http_req_failed: ['rate<0.01'],
              errors: ['rate<0.05'],
            },
          };
          
          const BASE_URL = '${{ steps.set-urls.outputs.API_URL }}';
          
          export default function() {
            // Test critical API endpoints
            const endpoints = [
              '/health',
              '/api/v1/garden/data',
              '/api/v1/projects/',
              '/api/v1/skills/',
              '/api/v1/weather/current'
            ];
            
            for (const endpoint of endpoints) {
              const response = http.get(`${BASE_URL}${endpoint}`, {
                tags: { endpoint: endpoint },
              });
              
              check(response, {
                'status is 200': (r) => r.status === 200,
                'response time < 500ms': (r) => r.timings.duration < 500,
              });
              
              errorRate.add(response.status !== 200);
              responseTime.add(response.timings.duration);
              
              sleep(0.1);
            }
            
            sleep(1);
          }
          EOF
          
          k6 run --out json=api-performance-results.json api-perf-test.js
      
      - name: 📊 Parse API performance results
        id: api-results
        run: |
          # Extract key metrics from k6 results
          AVG_RESPONSE_TIME=$(jq -r '.metrics.http_req_duration.values.avg' api-performance-results.json)
          P95_RESPONSE_TIME=$(jq -r '.metrics.http_req_duration.values["p(95)"]' api-performance-results.json)
          ERROR_RATE=$(jq -r '.metrics.http_req_failed.values.rate * 100' api-performance-results.json)
          
          echo "avg_response_time=${AVG_RESPONSE_TIME}" >> $GITHUB_OUTPUT
          echo "p95_response_time=${P95_RESPONSE_TIME}" >> $GITHUB_OUTPUT
          echo "error_rate=${ERROR_RATE}" >> $GITHUB_OUTPUT
      
      - name: 📈 Create API performance report
        run: |
          cat > api-performance-report-${{ matrix.environment }}.md << EOF
          # ⚡ API Performance Report - ${{ matrix.environment }}
          
          **Date**: $(date)
          **API URL**: ${{ steps.set-urls.outputs.API_URL }}
          
          ## Performance Metrics
          
          | Metric | Value | Target |
          |--------|-------|--------|
          | Average Response Time | ${{ steps.api-results.outputs.avg_response_time }}ms | < 200ms |
          | 95th Percentile Response Time | ${{ steps.api-results.outputs.p95_response_time }}ms | < 500ms |
          | Error Rate | ${{ steps.api-results.outputs.error_rate }}% | < 1% |
          
          ## Analysis
          
          EOF
          
          # Add performance analysis
          if (( $(echo "${{ steps.api-results.outputs.avg_response_time }}" '>' 200 | bc -l) )); then
            echo "- ⚠️ Average response time exceeds target. Consider optimizing database queries or adding caching." >> api-performance-report-${{ matrix.environment }}.md
          fi
          
          if (( $(echo "${{ steps.api-results.outputs.error_rate }}" '>' 1 | bc -l) )); then
            echo "- 🚨 Error rate is above acceptable threshold. Investigate failing endpoints." >> api-performance-report-${{ matrix.environment }}.md
          fi
      
      - name: 📤 Upload API performance results
        uses: actions/upload-artifact@v3
        with:
          name: api-performance-results-${{ matrix.environment }}
          path: |
            api-performance-results.json
            api-performance-report-${{ matrix.environment }}.md
          retention-days: 30

  # ===================================
  # Bundle Size Analysis
  # ===================================
  
  bundle-analysis:
    name: 📦 Bundle Size Analysis
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'push' ||
      github.event.inputs.test_type == 'bundle-analysis' ||
      github.event.inputs.test_type == 'full-suite'
    
    defaults:
      run:
        working-directory: frontend
    
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Need full history for comparison
      
      - name: 🟢 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json
      
      - name: 📦 Install dependencies
        run: npm ci
      
      - name: 🏗️ Build application
        run: npm run build
        env:
          NODE_ENV: production
      
      - name: 📊 Analyze bundle size
        run: node ../scripts/check-bundle-size.js
      
      - name: 📈 Bundle size tracking
        uses: preactjs/compressed-size-action@v2
        with:
          repo-token: '${{ secrets.GITHUB_TOKEN }}'
          pattern: 'frontend/dist/**/*.{js,css,html}'
          exclude: '{**/*.map,**/node_modules/**}'
          minimum-change-threshold: 100
      
      - name: 🔍 Generate bundle analysis
        run: |
          npx vite-bundle-analyzer --mode production --out ../bundle-analysis.html
          
          # Create bundle report
          cat > bundle-report.md << EOF
          # 📦 Bundle Analysis Report
          
          **Date**: $(date)
          **Commit**: ${{ github.sha }}
          
          ## Bundle Sizes
          
          EOF
          
          # Add bundle size information (this would be populated by the check-bundle-size.js script)
          echo "See detailed analysis in bundle-analysis.html" >> bundle-report.md
      
      - name: 📤 Upload bundle analysis
        uses: actions/upload-artifact@v3
        with:
          name: bundle-analysis
          path: |
            bundle-analysis.html
            frontend/bundle-report.md
          retention-days: 30

  # ===================================
  # Performance Regression Detection
  # ===================================
  
  performance-regression:
    name: 🔄 Performance Regression Detection
    runs-on: ubuntu-latest
    needs: [lighthouse-audit, api-performance]
    if: always()
    
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4
      
      - name: 📊 Download performance results
        uses: actions/download-artifact@v3
        with:
          path: ./performance-results
      
      - name: 🔍 Compare with baseline
        run: |
          # This would implement performance regression detection
          # by comparing current results with historical baselines
          echo "Performance regression analysis would be implemented here"
          echo "Comparing with previous runs and alerting on significant changes"
      
      - name: 📈 Generate trend report
        run: |
          cat > performance-trends.md << EOF
          # 📈 Performance Trends Report
          
          **Date**: $(date)
          
          ## Trend Analysis
          
          - Performance trends over time would be analyzed here
          - Regression detection results would be displayed
          - Recommendations for improvement would be generated
          
          EOF
      
      - name: 📤 Upload trend report
        uses: actions/upload-artifact@v3
        with:
          name: performance-trends
          path: performance-trends.md

  # ===================================
  # Performance Dashboard Update
  # ===================================
  
  update-dashboard:
    name: 📊 Update Performance Dashboard
    runs-on: ubuntu-latest
    needs: [lighthouse-audit, api-performance, bundle-analysis]
    if: always()
    
    steps:
      - name: 📊 Update performance dashboard
        run: |
          echo "Updating performance monitoring dashboard..."
          echo "This would typically:"
          echo "  - Send metrics to monitoring system (Prometheus, DataDog, etc.)"
          echo "  - Update dashboard with latest performance data"
          echo "  - Set up alerts for performance regressions"
      
      - name: 📢 Notify team of performance status
        uses: 8398a7/action-slack@v3
        with:
          status: custom
          custom_payload: |
            {
              "text": "📊 Performance Monitoring Complete - Digital Greenhouse",
              "attachments": [{
                "color": "good",
                "fields": [
                  {"title": "Lighthouse Audit", "value": "${{ needs.lighthouse-audit.result }}", "short": true},
                  {"title": "API Performance", "value": "${{ needs.api-performance.result }}", "short": true},
                  {"title": "Bundle Analysis", "value": "${{ needs.bundle-analysis.result }}", "short": true}
                ],
                "footer": "View detailed results in the Actions artifacts"
              }]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}